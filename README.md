# ğŸ“Š Ad Campaign Spend Tracker

A comprehensive **data engineering + BI portfolio project** that simulates ad campaign data across platforms like Google, Meta, and LinkedIn, and builds a full data pipeline from generation to dashboard.

## ğŸ¯ Project Overview

This project demonstrates end-to-end data engineering capabilities including:
- **Data Generation**: Automated fake data creation using Faker
- **Orchestration**: Airflow DAGs for pipeline automation
- **Data Warehouse**: Snowflake integration for data storage
- **Transformation**: dbt models for data modeling and transformation
- **Data Quality**: Great Expectations for validation
- **Testing**: PyTest for unit testing
- **Visualization**: Tableau dashboards for business intelligence

## ğŸ—ï¸ Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Data Gen      â”‚    â”‚    Airflow      â”‚    â”‚   Snowflake     â”‚
â”‚   (Faker)      â”‚â”€â”€â”€â–¶â”‚   Orchestration â”‚â”€â”€â”€â–¶â”‚   Data Lake     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                â”‚                       â”‚
                                â–¼                       â–¼
                       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                       â”‚      dbt        â”‚    â”‚   Great         â”‚
                       â”‚  Transformation â”‚    â”‚ Expectations    â”‚
                       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                â”‚                       â”‚
                                â–¼                       â–¼
                       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                       â”‚    Tableau      â”‚    â”‚     PyTest      â”‚
                       â”‚   Dashboard     â”‚    â”‚   Unit Tests    â”‚
                       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸ“ Project Structure

```
ad_campaign_spend_tracker/
â”œâ”€â”€ data/
â”‚   â””â”€â”€ raw/                         # Raw CSVs generated by Faker
â”‚       â”œâ”€â”€ ads_2025-08-15.csv      # Daily data files
â”‚       â””â”€â”€ ads_backfill.csv        # Historical backfill data
â”‚
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ generate_fake_ads.py         # Daily data generation (5k rows)
â”‚   â””â”€â”€ generate_backfill_ads.py    # Historical backfill (300k rows)
â”‚
â”œâ”€â”€ dags/
â”‚   â””â”€â”€ ad_data_generator_dag.py     # Airflow DAG for automation
â”‚
â”œâ”€â”€ dbt/                             # Data transformation layer
â”‚   â”œâ”€â”€ models/
â”‚   â”‚   â”œâ”€â”€ staging/
â”‚   â”‚   â”œâ”€â”€ intermediate/
â”‚   â”‚   â””â”€â”€ marts/
â”‚   â”œâ”€â”€ seeds/
â”‚   â””â”€â”€ snapshots/
â”‚
â”œâ”€â”€ ge/                              # Great Expectations config
â”‚   â”œâ”€â”€ expectations/
â”‚   â””â”€â”€ checkpoints/
â”‚
â”œâ”€â”€ tests/                           # Unit tests
â”‚   â””â”€â”€ test_metrics.py
â”‚
â”œâ”€â”€ dashboards/                      # Tableau workbooks
â”œâ”€â”€ notebooks/                       # Jupyter notebooks for EDA
â”œâ”€â”€ requirements.txt                 # Python dependencies
â””â”€â”€ README.md                        # This file
```

## ğŸš€ Quick Start

### 1. Environment Setup

```bash
# Clone the repository
git clone <your-repo-url>
cd ad_campaign_spend_tracker

# Create virtual environment
python -m venv venv
source venv/bin/activate  # On Windows: venv\Scripts\activate

# Install dependencies
pip install -r requirements.txt
```

### 2. Generate Sample Data

```bash
# Generate today's data (5,000 rows)
python scripts/generate_fake_ads.py

# Generate historical backfill (300,000 rows)
python scripts/generate_backfill_ads.py
```

### 3. Run Airflow DAG

```bash
# Start Airflow (if running locally)
airflow standalone

# The DAG will run automatically at 9:00 AM daily
# Or trigger manually from the Airflow UI
```

## ğŸ“Š Data Schema

### Raw Data Structure

| Column | Type | Description | Example |
|--------|------|-------------|---------|
| `campaign_id` | STRING | Unique campaign identifier | `a1b2c3d4` |
| `platform` | STRING | Ad platform | `Google`, `Facebook`, `LinkedIn` |
| `date` | DATE | Campaign date | `2025-08-15` |
| `geo` | STRING | Country code | `US`, `CA`, `GB` |
| `device` | STRING | Device type | `mobile`, `desktop`, `tablet` |
| `impressions` | INTEGER | Ad impressions | `15000` |
| `clicks` | INTEGER | Ad clicks | `450` |
| `spend_usd` | DECIMAL | Spend in USD | `125.50` |
| `conversions` | INTEGER | Conversions | `23` |

### Calculated Metrics

- **CTR (Click-Through Rate)**: `clicks / impressions`
- **CPC (Cost Per Click)**: `spend_usd / clicks`
- **ROAS (Return on Ad Spend)**: `conversions / spend_usd`
- **Conversion Rate**: `conversions / clicks`

## ğŸ”§ Configuration

### Environment Variables

Create a `.env` file in the root directory:

```bash
# Snowflake Configuration
SNOWFLAKE_ACCOUNT=your_account
SNOWFLAKE_USER=your_username
SNOWFLAKE_PASSWORD=your_password
SNOWFLAKE_WAREHOUSE=your_warehouse
SNOWFLAKE_DATABASE=your_database
SNOWFLAKE_SCHEMA=your_schema

# Airflow Configuration
AIRFLOW_HOME=/path/to/airflow
```

### dbt Configuration

The dbt project will be configured in the next phase with:
- Snowflake connection
- Model configurations
- Test definitions
- Documentation

## ğŸ“ˆ Data Pipeline

### Daily Workflow

1. **9:00 AM**: Airflow DAG triggers
2. **Data Generation**: Creates 5,000 rows of fake ad data
3. **File Storage**: Saves CSV to `data/raw/ads_YYYY-MM-DD.csv`
4. **Logging**: Records success/failure and file path
5. **Next Steps**: Ready for Snowflake ingestion and dbt transformation

### Data Quality Checks

- No null values in primary keys
- CTR must be < 1 (100%)
- Clicks cannot exceed impressions
- CPC must be positive
- Date ranges are valid

## ğŸ§ª Testing

### Unit Tests

```bash
# Run all tests
pytest tests/

# Run with coverage
pytest tests/ --cov=scripts --cov-report=html

# Run specific test file
pytest tests/test_metrics.py -v
```

### Data Validation

Great Expectations will validate:
- Data completeness
- Data types
- Business rules
- Statistical distributions

## ğŸ“Š Dashboard Features

The Tableau dashboard will include:
- **Platform Performance**: Spend, CTR, CPC by platform
- **Geographic Analysis**: Performance by country/region
- **Device Breakdown**: Mobile vs desktop performance
- **Trend Analysis**: Daily/weekly performance trends
- **ROI Metrics**: ROAS and conversion tracking
- **Alerting**: Anomaly detection for spend spikes

## ğŸš§ Next Steps

### Phase 3: Snowflake Integration
- Set up Snowflake connection
- Create staging tables
- Load daily CSV data

### Phase 4: dbt Models
- Staging models for data cleaning
- Intermediate models for metrics calculation
- Mart models for business intelligence

### Phase 5: Data Quality
- Great Expectations validation suites
- Automated quality checks
- Alerting for data issues

### Phase 6: Testing & Validation
- Unit tests for all functions
- Integration tests for the pipeline
- Performance testing

### Phase 7: Dashboard
- Tableau connection to Snowflake
- Interactive visualizations
- Automated refresh

## ğŸ¤ Contributing

1. Fork the repository
2. Create a feature branch
3. Make your changes
4. Add tests for new functionality
5. Submit a pull request

## ğŸ“ License

This project is for portfolio demonstration purposes. Feel free to use and modify for your own projects.

## ğŸ”— Contact

- **Author**: Data Engineer Portfolio
- **Project**: Ad Campaign Spend Tracker
- **Date**: 2025

---

**Happy Data Engineering! ğŸš€ğŸ“Š**
