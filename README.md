# ğŸ“Š Ad Campaign Spend Tracker

A comprehensive **data engineering + BI portfolio project** that simulates ad campaign data across platforms like Google, Meta, and LinkedIn, and builds a full data pipeline from generation to dashboard.

## ğŸ¯ Project Overview

This project demonstrates end-to-end data engineering capabilities including:
- **Data Generation**: Automated fake data creation using Faker (300k+ historical + daily incremental)
- **Orchestration**: Apache Airflow DAGs for pipeline automation
- **Data Warehouse**: Snowflake integration with programmatic access tokens
- **Data Management**: Smart data retention and duplicate prevention
- **Transformation**: dbt models for data modeling and transformation (next phase)
- **Data Quality**: Great Expectations for validation (future phase)
- **Testing**: PyTest for unit testing (future phase)
- **Visualization**: Tableau dashboards for business intelligence (future phase)

## ğŸ—ï¸ Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Data Gen      â”‚    â”‚    Airflow      â”‚    â”‚   Snowflake     â”‚
â”‚   (Faker)      â”‚â”€â”€â”€â–¶â”‚   Orchestration â”‚â”€â”€â”€â–¶â”‚   Data Lake     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                â”‚                       â”‚
                                â–¼                       â–¼
                       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                       â”‚      dbt        â”‚    â”‚   Great         â”‚
                       â”‚  Transformation â”‚    â”‚ Expectations    â”‚
                       â”‚   (Next Phase) â”‚    â”‚  (Future)       â”‚
                       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                â”‚                       â”‚
                                â–¼                       â–¼
                       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                       â”‚    Tableau      â”‚    â”‚     PyTest      â”‚
                       â”‚   Dashboard     â”‚    â”‚   Unit Tests    â”‚
                       â”‚   (Future)      â”‚    â”‚   (Future)      â”‚
                       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸ“ Project Structure

```
ad_campaign_spend_tracker/
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ raw/                         # Raw CSVs generated by Faker
â”‚   â”‚   â”œâ”€â”€ daily/                   # Daily data files by date
â”‚   â”‚   â”‚   â””â”€â”€ 2025/
â”‚   â”‚   â”‚       â””â”€â”€ 08/
â”‚   â”‚   â”‚           â””â”€â”€ ads_2025-08-15.csv
â”‚   â”‚   â””â”€â”€ historical/              # Historical backfill data
â”‚   â”‚       â””â”€â”€ ads_backfill.csv
â”‚   â””â”€â”€ archive/                     # Archived old data (90-day retention)
â”‚
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ generate_fake_ads.py         # Daily data generation (5k rows)
â”‚   â”œâ”€â”€ generate_backfill_ads.py     # Historical backfill (300k rows)
â”‚   â”œâ”€â”€ load_backfill_to_snowflake.py # Snowflake backfill loader
â”‚   â”œâ”€â”€ load_daily_snowflake.py      # Daily Snowflake loader with duplicate prevention
â”‚   â””â”€â”€ data_retention_manager.py    # Data retention and archiving
â”‚
â”œâ”€â”€ dags/
â”‚   â””â”€â”€ ad_data_generator_dag.py     # Unified Airflow DAG for entire pipeline
â”‚
â”œâ”€â”€ sql/
â”‚   â””â”€â”€ create_raw_table.sql         # Snowflake table schema
â”‚
â”œâ”€â”€ dbt/                             # Data transformation layer (next phase)
â”‚   â”œâ”€â”€ models/
â”‚   â”‚   â”œâ”€â”€ staging/
â”‚   â”‚   â”œâ”€â”€ intermediate/
â”‚   â”‚   â””â”€â”€ marts/
â”‚   â”œâ”€â”€ seeds/
â”‚   â””â”€â”€ snapshots/
â”‚
â”œâ”€â”€ ge/                              # Great Expectations config (future)
â”‚   â”œâ”€â”€ expectations/
â”‚   â””â”€â”€ checkpoints/
â”‚
â”œâ”€â”€ tests/                           # Unit tests (future)
â”‚   â””â”€â”€ test_metrics.py
â”‚
â”œâ”€â”€ dashboards/                      # Tableau workbooks (future)
â”œâ”€â”€ notebooks/                       # Jupyter notebooks for EDA
â”œâ”€â”€ requirements.txt                 # Python dependencies
â”œâ”€â”€ .env                            # Environment variables (create from template)
â””â”€â”€ README.md                        # This file
```

## ğŸš€ Quick Start

### 1. Environment Setup

```bash
# Clone the repository
git clone <your-repo-url>
cd ad_campaign_spend_tracker

# Create virtual environment
python -m venv venv
source venv/bin/activate  # On Windows: venv\Scripts\activate

# Install dependencies
pip install -r requirements.txt
```

### 2. Snowflake Configuration

Create a `.env` file from the template:

```bash
# Copy template
cp env_template_programmatic.txt .env

# Edit .env with your Snowflake credentials
SNOWFLAKE_ACCOUNT=your_account
SNOWFLAKE_USER=your_username
SNOWFLAKE_PRIVATE_KEY=your_private_key
SNOWFLAKE_WAREHOUSE=your_warehouse
SNOWFLAKE_DATABASE=your_database
SNOWFLAKE_SCHEMA=your_schema
```

### 3. Generate and Load Data

```bash
# Generate historical backfill (262,849 rows)
python scripts/generate_backfill_ads.py

# Load to Snowflake
python scripts/load_backfill_to_snowflake.py --force

# Generate today's data (5,000 rows)
python scripts/generate_fake_ads.py

# Load daily data to Snowflake
python scripts/load_daily_snowflake.py data/raw/daily/2025/08/ads_2025-08-15.csv
```

### 4. Run Airflow Pipeline

```bash
# Set Airflow home
export AIRFLOW_HOME=/path/to/project

# Test the DAG
airflow dags test ad_data_generator_dag 2025-08-15

# Start Airflow (if running locally)
airflow standalone
```

## ğŸ“Š Data Schema

### Raw Data Structure

| Column | Type | Description | Example |
|--------|------|-------------|---------|
| `campaign_id` | STRING | Unique campaign identifier | `Q1_Brand_0001` |
| `platform` | STRING | Ad platform | `Google`, `Facebook`, `LinkedIn` |
| `date` | DATE | Campaign date | `2025-08-15` |
| `geo` | STRING | Country code | `US`, `CA`, `GB` |
| `device` | STRING | Device type | `mobile`, `desktop`, `tablet` |
| `campaign_type` | STRING | Campaign objective | `brand_awareness`, `conversions` |
| `ad_format` | STRING | Ad format | `search`, `display`, `video` |
| `impressions` | INTEGER | Ad impressions | `15000` |
| `clicks` | INTEGER | Ad clicks | `450` |
| `spend_usd` | DECIMAL | Spend in USD | `125.50` |
| `conversions` | INTEGER | Conversions | `23` |

### Data Characteristics

- **Historical Data**: 262,849 rows (2025-04-17 to 2025-08-15)
- **Daily Data**: 5,000 rows per day
- **Platforms**: Google (45%), Facebook (25%), LinkedIn (15%), TikTok (10%), Twitter (5%)
- **Geographies**: 14 countries with realistic performance distribution
- **Campaign Types**: 7 types including brand awareness, conversions, traffic
- **Ad Formats**: 6 formats including search, display, video, social

## ğŸ”§ Configuration

### Environment Variables

The project uses programmatic access tokens for Snowflake authentication:

```bash
# Required Snowflake Configuration
SNOWFLAKE_ACCOUNT=your_account
SNOWFLAKE_USER=your_username
SNOWFLAKE_PRIVATE_KEY=your_private_key
SNOWFLAKE_WAREHOUSE=your_warehouse
SNOWFLAKE_DATABASE=your_database
SNOWFLAKE_SCHEMA=your_schema
```

### Data Retention Policy

- **Retention Period**: 90 days
- **Local Files**: Automatically archived to `data/archive/`
- **Snowflake Data**: Old records moved to `raw.ad_data_archive` table
- **Cleanup Frequency**: Daily via Airflow DAG

## ğŸ“ˆ Data Pipeline

### Daily Workflow (9:00 AM)

1. **Data Generation**: Creates 5,000 rows of realistic ad campaign data
2. **File Storage**: Saves CSV to `data/raw/daily/YYYY/MM/ads_YYYY-MM-DD.csv`
3. **Snowflake Loading**: Loads daily data with duplicate prevention
4. **Data Retention**: Runs cleanup to maintain 90-day retention policy
5. **Logging**: Comprehensive pipeline summary with XCom communication

### Smart Data Management

- **Duplicate Prevention**: Checks existing `campaign_id`s before loading
- **Incremental Loading**: Only adds new, unique records
- **Data Retention**: Automatic cleanup prevents infinite growth
- **Archive Strategy**: Preserves old data in archive tables

### Current Pipeline Status

âœ… **Phase 1**: Data Generation & Storage - **COMPLETE**
âœ… **Phase 2**: Airflow Orchestration - **COMPLETE**  
âœ… **Phase 3**: Snowflake Integration - **COMPLETE**
ğŸ”„ **Phase 4**: dbt Transformation - **NEXT**
â³ **Phase 5**: Data Quality (Great Expectations) - **PLANNED**
â³ **Phase 6**: Testing & Validation - **PLANNED**
â³ **Phase 7**: Dashboard (Tableau) - **PLANNED**

## ğŸ§ª Testing

### Current Testing

```bash
# Test Airflow DAG
airflow dags test ad_data_generator_dag 2025-08-15

# Test data retention
python scripts/data_retention_manager.py --dry-run

# Test duplicate cleanup
python scripts/load_daily_snowflake.py --cleanup
```

### Future Testing (Planned)

```bash
# Unit tests
pytest tests/

# Data validation
great_expectations checkpoint run

# Performance testing
python scripts/performance_test.py
```

## ğŸ“Š Current Data Volume

### Snowflake Table: `raw.ad_data`
- **Total Rows**: 262,849
- **Date Range**: 2025-04-17 to 2025-08-15
- **Total Spend**: $156+ million
- **Total Impressions**: 6.5+ billion
- **Total Clicks**: 82+ million
- **Total Conversions**: 2.2+ million

### Data Distribution
- **Platforms**: 5 major ad platforms
- **Countries**: 14 geographic markets
- **Devices**: Mobile (64%), Desktop (31%), Tablet (5%)
- **Campaign Types**: 7 campaign objectives
- **Ad Formats**: 6 ad format types

## ğŸš§ Next Steps

### Phase 4: dbt Transformation Models (IMMEDIATE)
- Set up dbt project with Snowflake connection
- Create staging models for data cleaning
- Build intermediate models for metrics calculation
- Develop mart models for business intelligence
- Implement incremental models for performance

### Phase 5: Data Quality & Validation
- Great Expectations validation suites
- Automated quality checks
- Statistical anomaly detection
- Data lineage tracking

### Phase 6: Testing & Performance
- Unit tests for all functions
- Integration tests for the pipeline
- Performance testing and optimization
- Load testing for large datasets

### Phase 7: Business Intelligence
- Tableau connection to Snowflake
- Interactive dashboards
- Automated refresh schedules
- Business user self-service

## ğŸ¯ Portfolio Highlights

This project demonstrates:
- **Real-world Data Engineering**: Production-ready pipeline architecture
- **Cloud Data Warehouse**: Snowflake integration with best practices
- **Data Orchestration**: Apache Airflow with complex task dependencies
- **Data Lifecycle Management**: Retention policies and archiving strategies
- **Scalable Architecture**: Handles 300k+ records with efficient processing
- **Professional Standards**: Error handling, logging, and monitoring

## ğŸ¤ Contributing

1. Fork the repository
2. Create a feature branch
3. Make your changes
4. Add tests for new functionality
5. Submit a pull request

## ğŸ“ License

This project is for portfolio demonstration purposes. Feel free to use and modify for your own projects.

## ğŸ”— Contact

- **Author**: Data Engineer Portfolio
- **Project**: Ad Campaign Spend Tracker
- **Date**: 2025
- **Status**: Active Development

---

**Happy Data Engineering! ğŸš€ğŸ“Š**
