# Ad Campaign Spend Tracker

A comprehensive **Data Engineering Business Intelligence Project** demonstrating end-to-end data pipeline development, from data generation to business intelligence.

## ğŸ“Š Project Overview

This project uses **Faker** to generate realistic ad campaign data, simulating a real-world **Ad Campaign Analytics** system that processes data from multiple advertising platforms (Google, Facebook, LinkedIn, TikTok, Twitter) to provide actionable insights for marketing teams.

### ğŸ¯ **Project Highlights**

- **End-to-End Data Pipeline**: Airflow â†’ Snowflake â†’ dbt â†’ Analytics
- **Real Business Intelligence**: 4.58B impressions, $109.6M spend analysis
- **Professional Star Schema**: Kimball methodology implementation
- **Data Quality Assurance**: Great Expectations + PyTest testing
- **Modern Data Stack**: Airflow, Snowflake, dbt, Python, Qlik Sense
- **Production-Ready Code**: Comprehensive testing, documentation, error handling
- **Advanced Orchestration**: Master DAG with email alerts & monitoring
- **Professional Visualizations**: DAG graphs, star schema diagrams, dashboard mockups

## ğŸ—ï¸ Architecture

### ğŸŒŸ **Star Schema Data Model**

Our data architecture follows the **Kimball Star Schema** methodology, featuring:

**ğŸ¯ Kimball Methodology Benefits:**
- **Business-First Design**: Aligned with business processes and user needs
- **Query Performance**: Optimized for analytical workloads and reporting
- **Scalability**: Efficient handling of large datasets and complex joins
- **Maintainability**: Clear separation of concerns and logical structure
- **Industry Standard**: Proven approach used by leading organizations

- **1 Fact Table**: `fact_ad_performance` - Central hub for all metrics
- **6 Dimension Tables**: Complete Kimball implementation with business logic
  - **dim_campaigns**: Campaign details, objectives, budget tiers, status
  - **dim_devices**: Device types, platforms, screen sizes, mobile classification
  - **dim_ad_formats**: Ad formats, video/static classification, platform compatibility
  - **dim_dates**: Date dimensions with calendar attributes
  - **dim_geography**: Geographic markets and country data
  - **dim_platforms**: Advertising platform information
- **Optimized Performance**: Indexed foreign keys, denormalized structure
- **Business Focus**: Aligned with marketing analytics requirements


### ğŸ“Š **Database ERD **

Here's a visual representation of the AdSpendIQ exported from DBeaver, illustrating the relationships between the fact, dimension, and mart tables:

![AdSpendIQ Star Schema ERD](star_schema_diagrams/adspendiq.png)

**ERD Features:**
- **Central Fact Table**: `fact_ad_performance` with all ad metrics
- **6 Dimension Tables**: Complete Kimball methodology implementation
- **3 Data Mart Tables**: Business intelligence aggregations
- **Proper Relationships**: All foreign key constraints and cardinality
- **Professional Design**: Clean, readable database structure

## ğŸ—ï¸ **Architecture**

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Data Sources  â”‚    â”‚   Apache        â”‚    â”‚   Snowflake     â”‚    â”‚   dbt           â”‚
â”‚   (Faker)       â”‚â”€â”€â”€â–¶â”‚   Airflow       â”‚â”€â”€â”€â–¶â”‚   Data          â”‚â”€â”€â”€â–¶â”‚   Transform     â”‚
â”‚                 â”‚    â”‚   Orchestration â”‚    â”‚   Warehouse     â”‚    â”‚   & Modeling   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                â”‚                       â”‚                       â”‚
                                â–¼                       â–¼                       â–¼
                       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                       â”‚   Data Quality  â”‚    â”‚   Data          â”‚    â”‚   Business      â”‚
                       â”‚   Validation    â”‚    â”‚   Retention     â”‚    â”‚   Intelligence  â”‚
                       â”‚   (Great        â”‚    â”‚   Management    â”‚    â”‚   (Qlik Sense)  â”‚
                       â”‚    Expectations â”‚    â”‚                 â”‚    â”‚                 â”‚
                       â”‚   + PyTest)     â”‚    â”‚                 â”‚    â”‚                 â”‚
                       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```



### ğŸš€ **Airflow DAG Visualizations**

Here are all the Airflow DAGs that orchestrate the AdSpendIQ data pipeline, plotted using Graphviz:

#### **Master Portfolio Pipeline DAG**
![Master Portfolio Pipeline](dag_visualizations/master_portfolio_pipeline_dag.png)

**Purpose**: Orchestrates the complete end-to-end data pipeline execution
**What it does**: Coordinates all other DAGs, manages dependencies, and ensures the entire data pipeline runs successfully from data generation to business intelligence

#### **Data Generation & Loading DAGs**

**Ad Data Generator DAG**
![Ad Data Generator DAG](dag_visualizations/ad_data_generator_dag.png)

**Purpose**: Generates and loads daily ad campaign data
**What it does**: 
- Generates realistic daily ad data (5,000+ records per day)
- Loads data to Snowflake data warehouse
- Applies data retention policies
- Logs pipeline execution summary

#### **Transformation & Analytics DAGs**

**DBT Transformation DAG**
![DBT Transformation DAG](dag_visualizations/dbt_transformation_dag.png)

**Purpose**: Transforms raw data into analytical models
**What it does**: 
- Builds the complete Kimball star schema
- Creates dimension and fact tables
- Generates business intelligence marts
- Ensures data model consistency

**Data Quality Validation DAG**
![Data Quality Validation DAG](dag_visualizations/data_quality_validation_dag.png)

**Purpose**: Ensures data quality and integrity
**What it does**: 
- Validates data using Great Expectations
- Checks business rules and data constraints
- Monitors data quality metrics
- Alerts on quality issues

**Analytics Testing DAG**
![Analytics Testing DAG](dag_visualizations/analytics_testing_dag.png)

**Purpose**: Validates analytics and business logic
**What it does**: 
- Runs comprehensive data tests
- Validates business metrics and KPIs
- Ensures data accuracy for reporting
- Performs end-to-end pipeline testing

#### **Monitoring & Alerting DAG**
![Monitoring & Alerting DAG](dag_visualizations/monitoring_alerting_dag.png)

**Purpose**: Monitors pipeline health and sends alerts
**What it does**: 
- Tracks pipeline execution status
- Monitors data quality metrics
- Sends email alerts for failures
- Provides pipeline health dashboard

**DAG Features:**
- **Complete Pipeline Coverage**: End-to-end data workflow orchestration
- **Professional Visualization**: Clear task dependencies and relationships
- **Production Ready**: Error handling, retries, and monitoring
- **Scalable Architecture**: Modular design for easy maintenance and extension
- **Business Focus**: Each DAG serves a specific business purpose

**ğŸ”„ How the DAGs Work Together:**
1. **Master DAG** orchestrates the entire pipeline execution
2. **Data Generation DAG** creates and loads daily ad data
3. **Data Quality DAG** validates data integrity and quality
4. **DBT Transformation DAG** builds the analytical data model
5. **Analytics Testing DAG** ensures business logic accuracy
6. **Monitoring DAG** tracks pipeline health and sends alerts

**ğŸ“Š Pipeline Flow**: Data Generation â†’ Quality Validation â†’ Transformation â†’ Analytics Testing â†’ Business Intelligence

## ğŸ“ˆ Current Data Volume

- **Total Records**: 84,000+ ad campaign records
- **Date Range**: Last 90 days (rolling retention)
- **Platforms**: Google, Facebook, LinkedIn, TikTok, Twitter
- **Geographies**: 14 major markets (US, CA, GB, DE, FR, AU, JP, IN, BR, MX, NL, IT, ES, SE)
- **Campaign Types**: 7 objectives (brand awareness, conversions, traffic, etc.)
- **Daily Volume**: 5,000 new records per day

## ğŸš€ **Project Status**

| Component | Status | Description |
|-----------|--------|-------------|
| **Data Generation** | âœ… **COMPLETE** | Realistic ad campaign data with business logic |
| **Airflow Orchestration** | âœ… **COMPLETE** | Daily pipeline with smart data management |
| **Snowflake Integration** | âœ… **COMPLETE** | Cloud data warehouse with optimized loading |
| **dbt Transformation** | âœ… **COMPLETE** | Complete Kimball star schema with 6 dimensions + 4 marts |
| **Data Quality Testing** | âœ… **COMPLETE** | Great Expectations + PyTest suite |
| **Business Intelligence** | âœ… **COMPLETE** | Portfolio showcase queries & analytics |
| **Documentation** | âœ… **COMPLETE** | Auto-generated dbt docs & project docs |
| **Qlik Sense Dashboard** | âœ… **COMPLETE** | Complete dashboard setup & documentation |
| **Star Schema Diagrams** | âœ… **COMPLETE** | Professional diagrams & documentation |
| **DAG Visualizations** | âœ… **COMPLETE** | Airflow DAG graph images |
| **Email Alerts** | âœ… **COMPLETE** | SMTP configuration & setup |
| **Great Expectations** | âœ… **COMPLETE** | Advanced data validation |
| **Unit Testing** | âœ… **COMPLETE** | Automated test coverage |



## ğŸ› ï¸ **Technology Stack**

### **Core Technologies**
- **Python 3.11+**: Data processing, API integration
- **Apache Airflow 3.0**: Workflow orchestration & DAG management
- **Snowflake**: Cloud data warehouse & data storage
- **dbt**: Data transformation & modeling (Kimball methodology)
- **Pandas**: Data manipulation & analysis
- **Qlik Sense**: Business intelligence & dashboard creation

### **Data Quality & Testing**
- **Great Expectations**: Data validation & quality assurance
- **PyTest**: Unit testing & test automation
- **Coverage**: Code coverage reporting

### **Infrastructure**
- **Virtual Environment**: Dependency management
- **Environment Variables**: Secure credential management
- **Logging**: Comprehensive pipeline monitoring
- **Email Alerts**: SMTP configuration for notifications
- **DAG Visualization**: Graphviz integration for Airflow DAGs

## ğŸ“ **Project Structure**

```
ad_campaign_spend_tracker/
â”œâ”€â”€ ğŸ“Š dags/                          # Airflow DAGs & Orchestration
â”‚   â”œâ”€â”€ ad_data_generator_dag.py     # Main pipeline orchestration
â”‚   â”œâ”€â”€ data_quality_validation_dag.py # Data quality validation
â”‚   â”œâ”€â”€ dbt_transformation_dag.py    # dbt transformation pipeline
â”‚   â”œâ”€â”€ analytics_testing_dag.py     # Analytics & testing pipeline
â”‚   â”œâ”€â”€ monitoring_alerting_dag.py   # Monitoring & alerting
â”‚   â””â”€â”€ master_portfolio_pipeline_dag.py # Master orchestration DAG
â”œâ”€â”€ ğŸ”§ scripts/                       # Data processing scripts
â”‚   â”œâ”€â”€ generate_fake_ads.py         # Daily data generation
â”‚   â”œâ”€â”€ generate_backfill_ads.py     # Historical data generation
â”‚   â”œâ”€â”€ load_backfill_to_snowflake.py # Initial data loading
â”‚   â”œâ”€â”€ load_daily_snowflake.py      # Daily incremental loading
â”‚   â””â”€â”€ data_retention_manager.py    # Data lifecycle management
â”œâ”€â”€ ğŸ—„ï¸ dbt/                          # Data transformation & modeling
â”‚   â”œâ”€â”€ models/                      # dbt models
â”‚   â”‚   â”œâ”€â”€ staging/                # Data cleaning & validation
â”‚   â”‚   â”œâ”€â”€ dimensions/             # Dimension tables
â”‚   â”‚   â””â”€â”€ marts/                  # Business intelligence marts
â”‚   â”œâ”€â”€ dbt_project.yml             # dbt configuration
â”‚   â””â”€â”€ profiles.yml                # Snowflake connection
â”œâ”€â”€ ğŸ§ª tests/                        # Test suite & coverage
â”‚   â”œâ”€â”€ test_data_generation.py     # Data generation tests
â”‚   â””â”€â”€ test_*.py                   # Additional test files
â”œâ”€â”€ ğŸ” great_expectations/           # Data quality validation
â”‚   â”œâ”€â”€ great_expectations.yml      # GE configuration
â”‚   â”œâ”€â”€ expectations/                # Data quality expectations
â”‚   â””â”€â”€ validate_ad_data.py         # Validation script
â”œâ”€â”€ ğŸŒŸ star_schema_diagrams/         # Star schema diagrams & documentation
â”‚   â”œâ”€â”€ star_schema_simple.png       # Overview diagram
â”‚   â”œâ”€â”€ star_schema_detailed.png     # Detailed field diagram
â”‚   â”œâ”€â”€ star_schema_diagram.md       # Mermaid diagram for GitHub
â”‚   â”œâ”€â”€ STAR_SCHEMA_GUIDE.md         # Complete usage guide
â”‚   â”œâ”€â”€ generate_star_schema.py      # Custom diagram generator
â”‚   â””â”€â”€ FOLDER_STRUCTURE.md          # Organization overview
â”œâ”€â”€ ğŸ—„ï¸ mysql_erd_setup/              # MySQL ERD setup & documentation
â”‚   â”œâ”€â”€ mysql_erd_schema.sql         # Complete MySQL schema script
â”‚   â”œâ”€â”€ MYSQL_ERD_GUIDE.md          # MySQL Workbench ERD guide
â”‚   â””â”€â”€ README.md                    # ERD setup documentation
â”œâ”€â”€ ğŸ“Š dag_visualizations/           # Airflow DAG visualizations
â”‚   â”œâ”€â”€ README.md                    # DAG visualization documentation
â”‚   â”œâ”€â”€ master_portfolio_pipeline_dag.png # Master orchestration DAG
â”‚   â”œâ”€â”€ ad_data_generator_dag.png   # Data generation pipeline
â”‚   â”œâ”€â”€ data_quality_validation_dag.png # Data quality pipeline
â”‚   â”œâ”€â”€ dbt_transformation_dag.png  # DBT transformation pipeline
â”‚   â”œâ”€â”€ analytics_testing_dag.png   # Analytics & testing pipeline
â”‚   â””â”€â”€ monitoring_alerting_dag.png # Monitoring & alerting pipeline
â”œâ”€â”€ ğŸ¨ qlik_sense_dashboard/         # Qlik Sense dashboard setup
â”‚   â”œâ”€â”€ README.md                    # Dashboard overview
â”‚   â”œâ”€â”€ QUICK_START.md               # Quick start guide
â”‚   â”œâ”€â”€ qlik_workbook_template.md    # Step-by-step app creation
â”‚   â”œâ”€â”€ advanced_calculations.md     # Advanced Qlik expressions
â”‚   â””â”€â”€ dashboard_mockup.md          # Visual layout guide

```

## ğŸ¨ **Portfolio Components**

### **ğŸŒŸ Complete Visual Portfolio**

Your AdSpendIQ project now includes a comprehensive visual portfolio showcasing:

- **ğŸ“Š Database ERD**: Professional Kimball star schema visualization
- **ğŸš€ Airflow DAGs**: Complete pipeline orchestration diagrams
- **ğŸŒŸ Star Schema Diagrams**: Detailed data model documentation
- **ğŸ¨ Qlik Sense Dashboard**: Business intelligence mockups
- **ğŸ“ˆ DAG Visualizations**: Workflow orchestration showcase

### **ğŸŒŸ Star Schema Diagrams**
- **Professional Visualizations**: PNG diagrams for presentations
- **GitHub Integration**: Mermaid diagrams for repositories
- **Custom Generation**: Python script for modifications
- **Complete Documentation**: Usage guides and examples
- **6 Dimension Tables**: Full Kimball implementation with business logic

### **ğŸ—„ï¸ MySQL ERD Setup**
- **Complete Schema Script**: Ready-to-run MySQL database creation
- **Multiple Tool Support**: Works with MySQL Workbench, DBeaver, and more
- **Professional ERD**: Star schema visualization with relationships
- **Portfolio Ready**: Perfect for showcasing database design skills

### **ğŸ“Š Airflow DAG Visualizations**
- **Complete Pipeline Views**: All 6 DAGs visualized with professional graphs
- **Workflow Orchestration**: Clear representation of data pipeline stages
- **Task Dependencies**: Visual mapping of complex workflow relationships
- **Portfolio Showcase**: Professional diagrams demonstrating orchestration skills
- **Professional Layout**: Clean, readable workflow representations
- **Production Ready**: Error handling, monitoring, and alerting visualization

### **ğŸ¨ Qlik Sense Dashboard**
- **Complete Setup Guide**: Step-by-step implementation
- **Advanced Calculations**: Sophisticated business metrics
- **Visual Mockups**: Professional dashboard layouts
- **Portfolio Ready**: Documentation and examples



### **ğŸ“§ Email Alert System**
- **SMTP Configuration**: Professional notification setup
- **Master DAG Integration**: Automated pipeline monitoring
- **Portfolio Demonstration**: Real-world alerting system

## ğŸš€ **Quick Start**

### **1. Environment Setup**
```bash
# Clone repository
git clone <your-repo-url>
cd ad_campaign_spend_tracker

# Create virtual environment
python -m venv venv
source venv/bin/activate  # On Windows: venv\Scripts\activate

# Install dependencies
pip install -r requirements.txt
pip install -r requirements-test.txt
```

### **2. Snowflake Configuration**
```bash
# Copy environment template
cp .env.example .env

# Edit .env with your Snowflake credentials
SNOWFLAKE_ACCOUNT=your_account
SNOWFLAKE_USER=your_username
SNOWFLAKE_PROGRAMMATIC_TOKEN=your_token
SNOWFLAKE_DATABASE=AD_CAMPAIGNS
SNOWFLAKE_SCHEMA=RAW
SNOWFLAKE_WAREHOUSE=COMPUTE_WH
```

### **3. Data Pipeline Execution**
```bash
# Generate and load initial data
python scripts/generate_backfill_ads.py
python scripts/load_backfill_to_snowflake.py

# Run daily pipeline
python scripts/generate_fake_ads.py
python scripts/load_daily_snowflake.py
```

### **4. Data Transformation**
```bash
# Navigate to dbt directory
cd dbt

# Install dbt dependencies
dbt deps

# Run transformations
dbt run

# Generate documentation
dbt docs generate
dbt docs serve
```

### **5. Run Analytics**
```bash
# Execute portfolio showcase queries
python run_portfolio_queries.py
```

## ğŸ§ª **Testing & Quality Assurance**

### **PyTest Test Suite**
```bash
# Run all tests
python run_tests.py

# Run specific test file
python -m pytest tests/ -v

# Run with coverage
python -m pytest tests/ --cov=scripts --cov=dbt --cov-report=html
```

### **Great Expectations Validation**
```bash
# Run data quality validation
python great_expectations/validate_ad_data.py
```

### **Test Coverage**
- **Data Generation**: 10 comprehensive tests
- **Business Logic**: Data quality rules validation
- **Data Types**: Schema validation
- **Value Ranges**: Business rule enforcement
- **Coverage Target**: 80%+ code coverage





## ğŸ“ˆ **Business Intelligence**

### **Key Metrics**
- **CTR (Click-Through Rate)**: Click performance
- **CPC (Cost Per Click)**: Cost efficiency
- **CVR (Conversion Rate)**: Conversion performance
- **ROAS (Return on Ad Spend)**: ROI measurement
- **CPM (Cost Per Mille)**: Impression cost

### **Analytics Capabilities**
- **Platform Performance**: Cross-platform comparison
- **Geographic Analysis**: Market performance insights
- **Campaign Effectiveness**: Objective-based analysis
- **Time Series Analysis**: Trend identification
- **Device Performance**: Cross-device optimization

## ğŸ¤ **Contributing**

This is a portfolio project demonstrating data engineering skills. For questions or feedback:

1. **Review the code**: All scripts are well-documented
2. **Run the tests**: Ensure quality with `python run_tests.py`
3. **Explore the data**: Use `run_portfolio_queries.py`
4. **Check documentation**: `dbt docs serve`

## ğŸ“„ **License**

This project is created for portfolio demonstration purposes. Feel free to use as a reference for your own projects.

---




