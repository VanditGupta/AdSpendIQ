# 📊 Ad Campaign Spend Tracker

A comprehensive **data engineering + BI portfolio project** that simulates ad campaign data across platforms like Google, Meta, and LinkedIn, and builds a full data pipeline from generation to dashboard.

## 🎯 Project Overview

This project demonstrates end-to-end data engineering capabilities including:
- **Data Generation**: Automated fake data creation using Faker (300k+ historical + daily incremental)
- **Orchestration**: Apache Airflow DAGs for pipeline automation
- **Data Warehouse**: Snowflake integration with programmatic access tokens
- **Data Management**: Smart data retention and duplicate prevention
- **Transformation**: dbt models for data modeling and transformation (next phase)
- **Data Quality**: Great Expectations for validation (future phase)
- **Testing**: PyTest for unit testing (future phase)
- **Visualization**: Tableau dashboards for business intelligence (future phase)

## 🏗️ Architecture

```
┌─────────────────┐    ┌─────────────────┐    ┌─────────────────┐
│   Data Gen      │    │    Airflow      │    │   Snowflake     │
│   (Faker)      │───▶│   Orchestration │───▶│   Data Lake     │
└─────────────────┘    └─────────────────┘    └─────────────────┘
                                │                       │
                                ▼                       ▼
                       ┌─────────────────┐    ┌─────────────────┐
                       │      dbt        │    │   Great         │
                       │  Transformation │    │ Expectations    │
                       │   (Next Phase) │    │  (Future)       │
                       └─────────────────┘    └─────────────────┘
                                │                       │
                                ▼                       ▼
                       ┌─────────────────┐    ┌─────────────────┐
                       │    Tableau      │    │     PyTest      │
                       │   Dashboard     │    │   Unit Tests    │
                       │   (Future)      │    │   (Future)      │
                       └─────────────────┘    └─────────────────┘
```

## 📁 Project Structure

```
ad_campaign_spend_tracker/
├── data/
│   ├── raw/                         # Raw CSVs generated by Faker
│   │   ├── daily/                   # Daily data files by date
│   │   │   └── 2025/
│   │   │       └── 08/
│   │   │           └── ads_2025-08-15.csv
│   │   └── historical/              # Historical backfill data
│   │       └── ads_backfill.csv
│   └── archive/                     # Archived old data (90-day retention)
│
├── scripts/
│   ├── generate_fake_ads.py         # Daily data generation (5k rows)
│   ├── generate_backfill_ads.py     # Historical backfill (300k rows)
│   ├── load_backfill_to_snowflake.py # Snowflake backfill loader
│   ├── load_daily_snowflake.py      # Daily Snowflake loader with duplicate prevention
│   └── data_retention_manager.py    # Data retention and archiving
│
├── dags/
│   └── ad_data_generator_dag.py     # Unified Airflow DAG for entire pipeline
│
├── sql/
│   └── create_raw_table.sql         # Snowflake table schema
│
├── dbt/                             # Data transformation layer (next phase)
│   ├── models/
│   │   ├── staging/
│   │   ├── intermediate/
│   │   └── marts/
│   ├── seeds/
│   └── snapshots/
│
├── ge/                              # Great Expectations config (future)
│   ├── expectations/
│   └── checkpoints/
│
├── tests/                           # Unit tests (future)
│   └── test_metrics.py
│
├── dashboards/                      # Tableau workbooks (future)
├── notebooks/                       # Jupyter notebooks for EDA
├── requirements.txt                 # Python dependencies
├── .env                            # Environment variables (create from template)
└── README.md                        # This file
```

## 🚀 Quick Start

### 1. Environment Setup

```bash
# Clone the repository
git clone <your-repo-url>
cd ad_campaign_spend_tracker

# Create virtual environment
python -m venv venv
source venv/bin/activate  # On Windows: venv\Scripts\activate

# Install dependencies
pip install -r requirements.txt
```

### 2. Snowflake Configuration

Create a `.env` file from the template:

```bash
# Copy template
cp env_template_programmatic.txt .env

# Edit .env with your Snowflake credentials
SNOWFLAKE_ACCOUNT=your_account
SNOWFLAKE_USER=your_username
SNOWFLAKE_PRIVATE_KEY=your_private_key
SNOWFLAKE_WAREHOUSE=your_warehouse
SNOWFLAKE_DATABASE=your_database
SNOWFLAKE_SCHEMA=your_schema
```

### 3. Generate and Load Data

```bash
# Generate historical backfill (262,849 rows)
python scripts/generate_backfill_ads.py

# Load to Snowflake
python scripts/load_backfill_to_snowflake.py --force

# Generate today's data (5,000 rows)
python scripts/generate_fake_ads.py

# Load daily data to Snowflake
python scripts/load_daily_snowflake.py data/raw/daily/2025/08/ads_2025-08-15.csv
```

### 4. Run Airflow Pipeline

```bash
# Set Airflow home
export AIRFLOW_HOME=/path/to/project

# Test the DAG
airflow dags test ad_data_generator_dag 2025-08-15

# Start Airflow (if running locally)
airflow standalone
```

## 📊 Data Schema

### Raw Data Structure

| Column | Type | Description | Example |
|--------|------|-------------|---------|
| `campaign_id` | STRING | Unique campaign identifier | `Q1_Brand_0001` |
| `platform` | STRING | Ad platform | `Google`, `Facebook`, `LinkedIn` |
| `date` | DATE | Campaign date | `2025-08-15` |
| `geo` | STRING | Country code | `US`, `CA`, `GB` |
| `device` | STRING | Device type | `mobile`, `desktop`, `tablet` |
| `campaign_type` | STRING | Campaign objective | `brand_awareness`, `conversions` |
| `ad_format` | STRING | Ad format | `search`, `display`, `video` |
| `impressions` | INTEGER | Ad impressions | `15000` |
| `clicks` | INTEGER | Ad clicks | `450` |
| `spend_usd` | DECIMAL | Spend in USD | `125.50` |
| `conversions` | INTEGER | Conversions | `23` |

### Data Characteristics

- **Historical Data**: 262,849 rows (2025-04-17 to 2025-08-15)
- **Daily Data**: 5,000 rows per day
- **Platforms**: Google (45%), Facebook (25%), LinkedIn (15%), TikTok (10%), Twitter (5%)
- **Geographies**: 14 countries with realistic performance distribution
- **Campaign Types**: 7 types including brand awareness, conversions, traffic
- **Ad Formats**: 6 formats including search, display, video, social

## 🔧 Configuration

### Environment Variables

The project uses programmatic access tokens for Snowflake authentication:

```bash
# Required Snowflake Configuration
SNOWFLAKE_ACCOUNT=your_account
SNOWFLAKE_USER=your_username
SNOWFLAKE_PRIVATE_KEY=your_private_key
SNOWFLAKE_WAREHOUSE=your_warehouse
SNOWFLAKE_DATABASE=your_database
SNOWFLAKE_SCHEMA=your_schema
```

### Data Retention Policy

- **Retention Period**: 90 days
- **Local Files**: Automatically archived to `data/archive/`
- **Snowflake Data**: Old records moved to `raw.ad_data_archive` table
- **Cleanup Frequency**: Daily via Airflow DAG

## 📈 Data Pipeline

### Daily Workflow (9:00 AM)

1. **Data Generation**: Creates 5,000 rows of realistic ad campaign data
2. **File Storage**: Saves CSV to `data/raw/daily/YYYY/MM/ads_YYYY-MM-DD.csv`
3. **Snowflake Loading**: Loads daily data with duplicate prevention
4. **Data Retention**: Runs cleanup to maintain 90-day retention policy
5. **Logging**: Comprehensive pipeline summary with XCom communication

### Smart Data Management

- **Duplicate Prevention**: Checks existing `campaign_id`s before loading
- **Incremental Loading**: Only adds new, unique records
- **Data Retention**: Automatic cleanup prevents infinite growth
- **Archive Strategy**: Preserves old data in archive tables

### Current Pipeline Status

✅ **Phase 1**: Data Generation & Storage - **COMPLETE**
✅ **Phase 2**: Airflow Orchestration - **COMPLETE**  
✅ **Phase 3**: Snowflake Integration - **COMPLETE**
🔄 **Phase 4**: dbt Transformation - **NEXT**
⏳ **Phase 5**: Data Quality (Great Expectations) - **PLANNED**
⏳ **Phase 6**: Testing & Validation - **PLANNED**
⏳ **Phase 7**: Dashboard (Tableau) - **PLANNED**

## 🧪 Testing

### Current Testing

```bash
# Test Airflow DAG
airflow dags test ad_data_generator_dag 2025-08-15

# Test data retention
python scripts/data_retention_manager.py --dry-run

# Test duplicate cleanup
python scripts/load_daily_snowflake.py --cleanup
```

### Future Testing (Planned)

```bash
# Unit tests
pytest tests/

# Data validation
great_expectations checkpoint run

# Performance testing
python scripts/performance_test.py
```

## 📊 Current Data Volume

### Snowflake Table: `raw.ad_data`
- **Total Rows**: 262,849
- **Date Range**: 2025-04-17 to 2025-08-15
- **Total Spend**: $156+ million
- **Total Impressions**: 6.5+ billion
- **Total Clicks**: 82+ million
- **Total Conversions**: 2.2+ million

### Data Distribution
- **Platforms**: 5 major ad platforms
- **Countries**: 14 geographic markets
- **Devices**: Mobile (64%), Desktop (31%), Tablet (5%)
- **Campaign Types**: 7 campaign objectives
- **Ad Formats**: 6 ad format types

## 🚧 Next Steps

### Phase 4: dbt Transformation Models (IMMEDIATE)
- Set up dbt project with Snowflake connection
- Create staging models for data cleaning
- Build intermediate models for metrics calculation
- Develop mart models for business intelligence
- Implement incremental models for performance

### Phase 5: Data Quality & Validation
- Great Expectations validation suites
- Automated quality checks
- Statistical anomaly detection
- Data lineage tracking

### Phase 6: Testing & Performance
- Unit tests for all functions
- Integration tests for the pipeline
- Performance testing and optimization
- Load testing for large datasets

### Phase 7: Business Intelligence
- Tableau connection to Snowflake
- Interactive dashboards
- Automated refresh schedules
- Business user self-service

## 🎯 Portfolio Highlights

This project demonstrates:
- **Real-world Data Engineering**: Production-ready pipeline architecture
- **Cloud Data Warehouse**: Snowflake integration with best practices
- **Data Orchestration**: Apache Airflow with complex task dependencies
- **Data Lifecycle Management**: Retention policies and archiving strategies
- **Scalable Architecture**: Handles 300k+ records with efficient processing
- **Professional Standards**: Error handling, logging, and monitoring

## 🤝 Contributing

1. Fork the repository
2. Create a feature branch
3. Make your changes
4. Add tests for new functionality
5. Submit a pull request

## 📝 License

This project is for portfolio demonstration purposes. Feel free to use and modify for your own projects.

## 🔗 Contact

- **Author**: Data Engineer Portfolio
- **Project**: Ad Campaign Spend Tracker
- **Date**: 2025
- **Status**: Active Development

---

**Happy Data Engineering! 🚀📊**
