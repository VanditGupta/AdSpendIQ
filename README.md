# 📊 Ad Campaign Spend Tracker

A comprehensive **data engineering + BI portfolio project** that simulates ad campaign data across platforms like Google, Meta, and LinkedIn, and builds a full data pipeline from generation to dashboard.

## 🎯 Project Overview

This project demonstrates end-to-end data engineering capabilities including:
- **Data Generation**: Automated fake data creation using Faker
- **Orchestration**: Airflow DAGs for pipeline automation
- **Data Warehouse**: Snowflake integration for data storage
- **Transformation**: dbt models for data modeling and transformation
- **Data Quality**: Great Expectations for validation
- **Testing**: PyTest for unit testing
- **Visualization**: Tableau dashboards for business intelligence

## 🏗️ Architecture

```
┌─────────────────┐    ┌─────────────────┐    ┌─────────────────┐
│   Data Gen      │    │    Airflow      │    │   Snowflake     │
│   (Faker)      │───▶│   Orchestration │───▶│   Data Lake     │
└─────────────────┘    └─────────────────┘    └─────────────────┘
                                │                       │
                                ▼                       ▼
                       ┌─────────────────┐    ┌─────────────────┐
                       │      dbt        │    │   Great         │
                       │  Transformation │    │ Expectations    │
                       └─────────────────┘    └─────────────────┘
                                │                       │
                                ▼                       ▼
                       ┌─────────────────┐    ┌─────────────────┐
                       │    Tableau      │    │     PyTest      │
                       │   Dashboard     │    │   Unit Tests    │
                       └─────────────────┘    └─────────────────┘
```

## 📁 Project Structure

```
ad_campaign_spend_tracker/
├── data/
│   └── raw/                         # Raw CSVs generated by Faker
│       ├── ads_2025-08-15.csv      # Daily data files
│       └── ads_backfill.csv        # Historical backfill data
│
├── scripts/
│   ├── generate_fake_ads.py         # Daily data generation (5k rows)
│   └── generate_backfill_ads.py    # Historical backfill (300k rows)
│
├── dags/
│   └── ad_data_generator_dag.py     # Airflow DAG for automation
│
├── dbt/                             # Data transformation layer
│   ├── models/
│   │   ├── staging/
│   │   ├── intermediate/
│   │   └── marts/
│   ├── seeds/
│   └── snapshots/
│
├── ge/                              # Great Expectations config
│   ├── expectations/
│   └── checkpoints/
│
├── tests/                           # Unit tests
│   └── test_metrics.py
│
├── dashboards/                      # Tableau workbooks
├── notebooks/                       # Jupyter notebooks for EDA
├── requirements.txt                 # Python dependencies
└── README.md                        # This file
```

## 🚀 Quick Start

### 1. Environment Setup

```bash
# Clone the repository
git clone <your-repo-url>
cd ad_campaign_spend_tracker

# Create virtual environment
python -m venv venv
source venv/bin/activate  # On Windows: venv\Scripts\activate

# Install dependencies
pip install -r requirements.txt
```

### 2. Generate Sample Data

```bash
# Generate today's data (5,000 rows)
python scripts/generate_fake_ads.py

# Generate historical backfill (300,000 rows)
python scripts/generate_backfill_ads.py
```

### 3. Run Airflow DAG

```bash
# Start Airflow (if running locally)
airflow standalone

# The DAG will run automatically at 9:00 AM daily
# Or trigger manually from the Airflow UI
```

## 📊 Data Schema

### Raw Data Structure

| Column | Type | Description | Example |
|--------|------|-------------|---------|
| `campaign_id` | STRING | Unique campaign identifier | `a1b2c3d4` |
| `platform` | STRING | Ad platform | `Google`, `Facebook`, `LinkedIn` |
| `date` | DATE | Campaign date | `2025-08-15` |
| `geo` | STRING | Country code | `US`, `CA`, `GB` |
| `device` | STRING | Device type | `mobile`, `desktop`, `tablet` |
| `impressions` | INTEGER | Ad impressions | `15000` |
| `clicks` | INTEGER | Ad clicks | `450` |
| `spend_usd` | DECIMAL | Spend in USD | `125.50` |
| `conversions` | INTEGER | Conversions | `23` |

### Calculated Metrics

- **CTR (Click-Through Rate)**: `clicks / impressions`
- **CPC (Cost Per Click)**: `spend_usd / clicks`
- **ROAS (Return on Ad Spend)**: `conversions / spend_usd`
- **Conversion Rate**: `conversions / clicks`

## 🔧 Configuration

### Environment Variables

Create a `.env` file in the root directory:

```bash
# Snowflake Configuration
SNOWFLAKE_ACCOUNT=your_account
SNOWFLAKE_USER=your_username
SNOWFLAKE_PASSWORD=your_password
SNOWFLAKE_WAREHOUSE=your_warehouse
SNOWFLAKE_DATABASE=your_database
SNOWFLAKE_SCHEMA=your_schema

# Airflow Configuration
AIRFLOW_HOME=/path/to/airflow
```

### dbt Configuration

The dbt project will be configured in the next phase with:
- Snowflake connection
- Model configurations
- Test definitions
- Documentation

## 📈 Data Pipeline

### Daily Workflow

1. **9:00 AM**: Airflow DAG triggers
2. **Data Generation**: Creates 5,000 rows of fake ad data
3. **File Storage**: Saves CSV to `data/raw/ads_YYYY-MM-DD.csv`
4. **Logging**: Records success/failure and file path
5. **Next Steps**: Ready for Snowflake ingestion and dbt transformation

### Data Quality Checks

- No null values in primary keys
- CTR must be < 1 (100%)
- Clicks cannot exceed impressions
- CPC must be positive
- Date ranges are valid

## 🧪 Testing

### Unit Tests

```bash
# Run all tests
pytest tests/

# Run with coverage
pytest tests/ --cov=scripts --cov-report=html

# Run specific test file
pytest tests/test_metrics.py -v
```

### Data Validation

Great Expectations will validate:
- Data completeness
- Data types
- Business rules
- Statistical distributions

## 📊 Dashboard Features

The Tableau dashboard will include:
- **Platform Performance**: Spend, CTR, CPC by platform
- **Geographic Analysis**: Performance by country/region
- **Device Breakdown**: Mobile vs desktop performance
- **Trend Analysis**: Daily/weekly performance trends
- **ROI Metrics**: ROAS and conversion tracking
- **Alerting**: Anomaly detection for spend spikes

## 🚧 Next Steps

### Phase 3: Snowflake Integration
- Set up Snowflake connection
- Create staging tables
- Load daily CSV data

### Phase 4: dbt Models
- Staging models for data cleaning
- Intermediate models for metrics calculation
- Mart models for business intelligence

### Phase 5: Data Quality
- Great Expectations validation suites
- Automated quality checks
- Alerting for data issues

### Phase 6: Testing & Validation
- Unit tests for all functions
- Integration tests for the pipeline
- Performance testing

### Phase 7: Dashboard
- Tableau connection to Snowflake
- Interactive visualizations
- Automated refresh

## 🤝 Contributing

1. Fork the repository
2. Create a feature branch
3. Make your changes
4. Add tests for new functionality
5. Submit a pull request

## 📝 License

This project is for portfolio demonstration purposes. Feel free to use and modify for your own projects.

## 🔗 Contact

- **Author**: Data Engineer Portfolio
- **Project**: Ad Campaign Spend Tracker
- **Date**: 2025

---

**Happy Data Engineering! 🚀📊**
